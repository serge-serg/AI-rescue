export default function Scenarios() {
  return (
    <>
      <h1>Will Superintelligence Become the Great Filter for Humanity?</h1>
      <h2>Пустая вселенная</h2>
      <p>В 1950 году, во время обеденного разговора с коллегами в Лос-Аламосской национальной лаборатории, выдающемуся физику Энрико Ферми пришёл в голову, казалось бы, простой вопрос: &laquo;Где все?&raquo;. Этот вопрос касался отсутствия наблюдаемых следов внеземных цивилизаций, несмотря на огромное количество звёзд и потенциально обитаемых планет во Вселенной. Такие цивилизации, будучи гораздо старше нашей и далеко обогнав нас в развитии, должны были бы уже колонизировать значительную часть галактики или оставить заметные следы своего существования. Однако мы не наблюдаем ни того, ни другого. Это противоречие стало известно как <em>парадокс Ферми</em>.</p>
      <p>Вопрос, заданный Ферми, стимулировал множество дискуссий и гипотез в научном сообществе. В самом деле, за последние годы было обнаружено множество так наз. <em>экзопланет</em> &mdash; небесных тел, сходных по размеру и природным условиям с Землёй. И это, судя по всему &mdash; лишь вершина айсберга. По расчётам космологов, их должно быть огромное количество только в нашей галактике. Причём некоторые находятся относительно недалеко от нашей планеты. В чём же дело?</p>
      <p>Учёные предлагали различные объяснения, от предположений о редкости жизни как явления во Вселенной до идей о том, что развитые цивилизации намеренно избегают контакта с нами.</p>
      <p>В 1998 году экономист Робин Хэнсон предложил собственную гипотезу, объясняющую Парадокс Ферми. В своей статье <a href="https://mason.gmu.edu/~rhanson/greatfilter.html"><em>The Great Filter &mdash; Are We Almost Past It?</em></a> он предположил, что на пути развития цивилизаций существует некий барьер или совокупность барьеров, которые большинство потенциальных цивилизаций не способны преодолеть. Этот барьер &laquo;отфильтровывает&raquo; их, не давая им достичь уровня, позволяющего оставить заметные следы во Вселенной.</p>
      <h2>Беспокойство особого характера</h2>
      <p>История человечества даёт немало поводов для беспокойства, связанного с идеей <em>The Great Filter</em>. Традиционно её применяли к таким экзистенциальным угрозам, как глобальная ядерная война, экологическая катастрофа, пандемии (включая искусственно созданные), или столкновение с Землёй крупного небесного тела, подобного <a href="https://www.newscientist.com/definition/chicxulub/">Чиксулубскому астероиду</a>, уничтожившему динозавров около 65&nbsp;млн. лет назад.</p>
      <p>В последние десятилетия этот список пополнился: по мере того, как создание Суперинтеллекта стало восприниматься всё более и более реальным, многие эксперты начали рассматривать его как одну из таких угроз.</p>
      <p>С одной стороны, в этих опасениях не было чего-то принципиально нового. Люди всегда испытывали интуитивный страх перед собственными творениями, превосходящими их силой. Причина этого страха кроется в лабиринтах нашей психологии, когда интуиция причудливо переплетается со свойством нашего ума во всём &mdash; будь то плохое или хорошее &mdash; видеть чьё-то намерение. Враждебные действия со стороны <em>наших собственных </em>созданий вызывают у нас ужас, поскольку доказывают нашу неспособность сделать собственные намерения безопасными для самих себя.</p>
      <p>С другой стороны, случай Суперинтеллекта особенный. На этот раз опасность исходит не от грубых, бесчувственных и безжалостных машин. Её источником служит нечто, превосходящее нас в том самом качестве, которое мы больше всего ценим в самих себе &mdash; способности мыслить. Это значит, что действия этой сущности будут преднамеренными и могут оказаться столь продуманными, что все наши попытки предусмотреть их провалятся. Напротив, эта сущность сможет предвидеть все наши намерения и предупредить их.
      {/*Напротив, эта сущность сможет предвидеть все наши намерения и предупредить их.*/}
      В конце концов, мы опасаемся того, что эта сущность сочтёт наше существование <em>нецелесообразным</em>. Она просто избавится от нас тем или иным способом и нас больше не будет.
      </p>
      <h2>Предупреждающие мнения</h2>
      <p>В общественном дискурсе проблема экзистенциального риска, исходящего от ИИ, представлена весьма обширно. Так, Mr. X (Илон Маск) заявлял, что &laquo;С искусственным интеллектом мы вызываем демона&raquo;. Джеймс Баррат, автор нашумевшей книги <em>Our Final Invention: Artificial Intelligence and the End of the Human Era</em> (2013), предостерегал, что &laquo;Мы приближаемся к тому моменту, когда искусственный интеллект будет таким умным, что сможет улучшать сам себя быстрее, чем мы можем его контролировать. Когда это произойдет, мы можем оказаться не готовы к последствиям&raquo;. Ник Бостром, философ и директор Института будущего человечества Оксфордского университета, в своей влиятельной книге <em>Superintelligence: Paths, Dangers, Strategies</em> (2014) утверждал, что создание суперинтеллекта может быть самым опасным проектом, с которым когда-либо сталкивалось человечество. По его мнению, ошибка в подходе к созданию всемогущего ИИ может оказаться фатальной. Стивен Хокинг, всемирно известный физик-теоретик (покинувший нас в 2018), также высказывал серьезные опасения по поводу развития ИИ. В интервью BBC в 2014 году он заявил: &laquo;Развитие полноценного искусственного интеллекта может означать конец человеческой расы&raquo;. Хокинг опасался, что ИИ может эволюционировать и перепроектировать себя с возрастающей скоростью, в то время как люди, ограниченные медленной биологической эволюцией, не смогут с ним конкурировать и будут вытеснены с исторической сцены.</p>
      <p>Подобные настроения широко распространены, и многие другие известные и влиятельные люди имеют похожую точку зрения. Впрочем, не все.</p>
      <h2>Обнадёживающие мнения</h2>
      <p>Так, Джефф Хокинс, основатель <a href="https://www.numenta.com/">Numenta</a>, лидера в разработке brain-inspired ИИ, убеждён, что мы в состоянии спроектировать ИИ, не представляющий для нас опасности. Он считает, что это &mdash; проблема дизайна и таким образом, всё в наших руках. В своей книге <em>On Intelligence</em> (2004) он пишет: &laquo;Мы не должны бояться создания машин умнее нас. Напротив, это наша судьба и наше достижение&raquo;. Хокинс верит, что глубокое понимание принципов работы человеческого мозга позволит нам создать безопасный и контролируемый ИИ.</p>
      <p>Рэй Курцвейл, футуролог и технический директор Google по машинному обучению и обработке естественного языка, известен своими оптимистичными прогнозами относительно ИИ. В своей книге <em>The Singularity Is Near</em> (2005) он утверждает: &laquo;Мы не превзойдем и не вытесним основные ценности человека, а скорее усилим эти ценности и распространим их на вселенную&raquo;. Показательно использование в этом утверждении местоимения &laquo;мы&raquo;. Курцвейл часто выражает идею, что будущее &mdash; это не противостояние людей и ИИ, а их объединение. Он предвидит мир, где человеческий интеллект усиливается и расширяется с помощью ИИ, а не заменяется им.</p>
      <p>Марк Цукерберг, основатель Meta, также настроен позитивно (как всегда). В 2017 году он заявил: &laquo;Я думаю, что вы можете построить вещи и мир становится лучше. Но с ИИ особенно, я очень оптимистичен&raquo;.</p>
      <p>Демис Хассабис, соучредитель и генеральный директор DeepMind, присоединяется к этим обнадёживающим мнениям и верит в возможность создания безопасного и полезного для человечества ИИ. В интервью The Guardian он сказал: &laquo;Я думаю, что ИИ будет одним из самых позитивных преобразований в истории человечества&raquo;. Впрочем, он тут же добавил, что &laquo;... но мы должны делать это правильно&raquo;.</p>
      <h2>Суперинтеллект, неотделимый от человеческой цивилизации</h2>
      <p>Итак, ст&oacute;ит ли нам бояться, что Сверхразум окажется Великим Фильтром, через который человечество не сможет пройти, или подобные опасения преувеличены?</p>
      <p>Мы считаем, что эта опасность вполне реальна и, по всей вероятности, более серьезна, чем все предыдущие. Она реальна потому, что Суперинтеллект будет обладать могуществом, достаточным, чтобы прекратить существование нашего вида или сделать его полностью зависимым от своей воли. В настоящее время у нас нет никакой гарантии того, что мы сможем эффективно противодействовать подобным его намерениям. Мы даже не можем быть уверены, что мы сможем понять эти намерения. По всей видимости, мы также не сможем создать надёжный механизм изоляции Суперинтеллекта от ресурсов, обеспечивающих нашу жизнедеятельность. Если он будет создан, он станет частью нашей человеческой цивилизации, очень быстро проникнув во все её области и получив доступ к её наиболее чувствительным точкам. Собственно, его предназначение и будет в том, чтобы стать этой частью. Как мы писали в разделе <u>Why We Won&apos;t Refuse Creating Superintelligence</u>, он нужен нам для того, чтобы решить свои фундаментальные проблемы, с которыми не можем справиться самостоятельно. Это значит, что он сможет воздействовать на все аспекты нашего общества &mdash; материальные, технические, политические, психологические и т.д. Он станет повсеместным, всё видящим и всё о нас знающим. Ближайшая, но всё ещё очень слабая аналогия &mdash; это интернет. Он проник повсюду и стал не только основой нашей коммуникационной инфраструктуры, но и неотъемлемой составляющей нашей общественной ткани. Но интернет, в отличие от Суперинтеллекта, не может принимать решений, определяющих судьбу нашего вида.</p>
      <p>Тем самым, вопрос сводится к тому, станет ли Суперинтеллект органичной частью человечества, воспринимающей его устремления как собственные, либо окажется отчуждённой от нас, но многократно превосходящей нас силою, сущностью.</p>
      <p>Если случится второе, то опасность того, что он окажется для человечества непреодолимым Великим Фильтром, чрезвычайно высока.</p>
      <h2>Filtering Humanity Out</h2>
        <p>Экзистенциальный риск со стороны Суперинтеллекта не является для нас неким неудачным стечением обстоятельств. Он есть побочный эффект достигнутой нами технологической мощи и неотделим от нее. Этот риск закономерен и неизбежен.</p>
        <p>Мы достигли этой мощи благодаря уникальной способности нашего разума понимать закономерности окружающего мира. К сожалению, обрести мощь этому разуму проще, нежели мудрость. В этом смысле создание Сверхразума станет проверкой нашей антропологической зрелости, и второй попытки у нас, скорее всего, не будет.</p>
        <h2>Обречены ли мы?</h2>
        <p>Может случиться и так, что нам не суждено пройти Великий Фильтр, как бы мы ни старались. Возможно, у нас есть некий естественный предел онтологического характера. Он не позволит нам стать теми, кем мы стать <em>не можем</em>, как не можем мы достичь звёзд, как бы хорошо нам ни удавалось их рассмотреть через наши самые мощные телескопы. Где-то в другой реальности мы, вероятно, приняли бы разумное стратегическое решение: обозначили границу разработки ИИ, и не делали бы ни шагу дальше, пока не получили бы надёжные свидетельства того, что этот следующий шаг не станет для нас роковым. Этот шаг был бы прозрачен для институционального контроля и общественность была бы о нём осведомлена. Никто не беспокоился бы о том, что за нашей спиной происходит нечто, что может иметь для всех нас необратимые последствия. Этого требует обычный здравый смысл, и к этому нас призывают многие обеспокоенные эксперты. Они <a href="https://futureoflife.org/ai/the-pause-letter-one-year-later/">делают это именно сейчас</a>, потому что <em>пока что</em> это имеет смысл. Мы ещё не прошли точку невозврата. Но мы приближаемся к ней. Это может произойти незаметно, и тогда уже никакие наши усилия не смогут ничего исправить.</p>
        <p>Где находится эта точка? Мы не знаем. Возможно, она лежит не во времени и пространстве, но внутри нас &mdash; внутри нашего понимания того, где предел допустимого риска. Если мы неспособны адекватно оценить этот риск, то не важно, сколько именно осталось до Суперинтеллекта. Апокалипсис может случиться даже до того, как он будет создан, если только мы не переключимся на <em>ту</em> реальность, где мы должны принять единственно разумное стратегическое решение.</p>
        <h2>Спорадическая шизофрения</h2>
        <p>Сценарий уничтожения человечества может включать в себя действия ИИ, вообще не обусловленные чьими-либо осознанными намерениями. Этот сценарий может быть запущен из-за сбоя или ошибки эмерджентного характера, что вызовет каскад неконтролируемых событий с катастрофическим исходом. Предвосхищение ошибок такого рода мы можем во множестве наблюдать при использовании ChatGPT, они известны как &laquo;галлюцинации&raquo;.</p>
        <p>Это выглядит пугающе. Не потому, конечно, что нерелевантная информация, выдаваемая ИИ-агентом конкретному юзеру, наносит сокрушительный удар по нашей цивилизации. Реальной проблемой является то, что создатели продуктов, основанных на LLM, признаются, что сами не понимают закономерностей продуцируемых их системами результатов. Никто не знает, почему ChatGPT, Claude или какой-нибудь ещё другой LLM-агент внезапно выдаёт ответ, выглядящий правдоподобным, но не имеющий ничего общего с реальностью. Это напоминает приступ шизофрении у кажущегося здоровым человека и невозможно предсказать, когда он повторится.</p>
        <h2>Динамика движения по порочному пути</h2>
        <p>Непонимание закономерностей такого поведения означает невозможность устранения его причины. Казалось бы, жёсткая конкуренция на рынке ИИ-продуктов должна заставить их разработчиков как можно быстрее устранить причины этих ошибок. Но этого не происходит по причинам, которые свидетельствуют о крайне опасной динамике, которую приобрела область разработки ИИ.</p>
        <p>Первое, что следует совершенно ясно осознавать, это то, что подобное поведение системы невозможно исправить. Этот её &laquo;баг&raquo; одновременно является её &laquo;фичей&raquo;. Он обусловлен непрозрачным операционным процессингом внутри LLM, ведущей себя как классический <em>Black&nbsp;Box</em>. LLM &mdash; это колоссальный набор цифр с некоторым сложным отношением между ними, а не программный код. Всё внутри неё вероятностно, а не дискретно. В этом состоит объяснение того, почему она никогда не выводит пользователю идентичный ответ.</p>
        <p>&nbsp;&mdash; неспособность общественности и правительственных институций своевременно среагировать на последствия массового нерегулируемого распространения продуктов на основе этого подхода. Большинство из тех, кто должен быть озабочен этой проблемой, не осознают, что происходит сейчас и не представляют, чем это может обернуться уже в ближайшем будущем. Неадекватное поведение систем, продвигаемых под маркой ИИ, закладывает бомбу замедленного действия под всю современную информационную инфраструктуру. Это значит, что опасности подвергается всё наше общество, все его социальные, производственные, финансовые, логистические и т.д. каналы.</p>
        <p>Наконец, разработчики LLM-систем, либо не понимают фундаментальной проблемы, стоящей за их подходом, либо преднамеренно вводят общественность в заблуждение. Они заявляют, что 1)&nbsp;этот подход способен привести к доброкачественному AGI и 2)&nbsp;этот процесс можно контролировать. Ни то, ни другое не являются правдой.</p>
        <h3>Во что верят адепты LLM</h3>
        <p>Илья Суцкевер, &laquo;крёстный отец&raquo; ChatGPT, в конце ноября 2023 <a href="https://www.youtube.com/watch?v=SEkGLj0bwAU&amp;t=619s&amp;pp=ygU8SWx5YSBTdXRza2V2ZXIg4oCUIFRoZSBFeGNpdGluZywgUGVyaWxvdXMgSm91cm5leSBUb3dhcmQgQUdJ">выступал на TED</a> с речью &ldquo;The Exciting, Perilous Journey Toward AGI&rdquo;, где изложил своё видение AGI. Эта речь наглядно демонстрирует образ мысли протагонистов LLM и даёт ценный инсайт того, куда может завести продолжение следования по этому пути. В частности, он сказал:</p>
        <p><em>Many of you may have spoken with a computer and a computer understood you and spoke back to you.</em></p>
        <p>Это утверждение ложно. Компьютер ничего не понимает, так как не обладает сознанием. Этот речевой оборот можно было бы посчитать метафорой, но продолжение речи не даёт повода на это надеяться.</p>
        <p><em>Artificial intelligence is nothing but digital brains inside large computers. That&rsquo;s what artificial intelligence is. Every single interesting AI that you&rsquo;ve seen is based on this idea.</em></p>
        <p>Это утверждение бессмысленно. Оно ничего не объясняет. По этой же причине оно очень удобно для производителей LLM-систем, поскольку его невалидность невозможно доказать. Тем самым оно легитимизирует произвольное использование лейбла &laquo;искусственный интеллект&raquo; для продуктов, чьё функционирование имеет с интеллектом, как фактом объективной реальности, мало или вообще ничего общего.</p>
        <p><em>When you speak to an AI chatbot, you very quickly see that it&rsquo;s not all there, that it&rsquo;s, you know, it understands mostly, sort of, but you can clearly see that there are so many things it cannot do and that there are some strange gaps. But this situation, I claim, is&nbsp;temporary.</em></p>
        <p>Это утверждение есть квинтэссенция LLM-подхода: агента, который не является, и <em>не может быть</em> разумным, можно сделать не менее &laquo;умным&raquo;, чем способный к резонированию человеческий ум.</p>
        <p><em>We call such an AI an&nbsp;AGI, artificial general intelligence, when we can say that the level at which we can teach the AI to do anything that, for example, I can do or someone else.</em></p>
        <p>Это утверждение некорректно. AGI сможет делать всё, что угодно, лучше людей не потому, что они его этому &laquo;научат&raquo;. Конечно, обучение даже самой интеллектуальной системы будет в той или иной мере необходимо. Но оно будет лишь одним из познавательных инструментов, позволяющих ей понять мир. Чтобы система могла его именно понимать, а не имитировать понимание, она должна обладать познавательным механизмом, позволяющим ей усваивать персональный опыт (этот концепт известен как <em>embodied cognition</em>). LLM никаким персональным опытом обладать не могут, поскольку не обладают субъектностью. Это &mdash; просто экземпляр ПО, существующий только во время сеанса взаимодействия с пользователем. Всё что они имеют &mdash; это массив мёртвых одномерных данных. Люди, в отличие от LLM, хранят данные в качестве воспоминаний о <em>событиях</em>. Эти события представлены в пространстве нашего ума в нескольких измерениях &mdash; эмоциональном, темпоральном, моральном и т.д.</p>
        <p>Эта разница объясняет фундаментальное различием между результатами статистического анализа и резонированием. LLM не способна делать то, для чего эволюция создала интеллект &mdash; предвосхищать будущее. Будущее есть экстраполяция прошлых и текущих событий, а не слов (это явление называется predictive coding). Слова вторичны. Они помечают события, но не устанавливают их сущностное содержание.</p>
        <p>Всемирно известный лингвист и мыслитель Noam&nbsp;Chomsky, a professor of linguistics Dr. Roberts, and Dr. Watumull (a director of artificial intelligence at a science and technology company) в эссе от March 8, 2023, <a href="https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html">опубликованном в The New York Times</a>, пишут, что</p>
        <p><em>Indeed, such programs are stuck in a prehuman or nonhuman phase of cognitive evolution. Their deepest flaw is the absence of the most critical capacity of any intelligence: to say not only what is the case, what was the case and what will be the case &mdash; that&rsquo;s description and prediction &mdash; but also what is not the case and what could and could not be the case. Those are the ingredients of explanation, the mark of true intelligence.</em></p>
        <p>Замечание о <em>prehuman or nonhuman phase of cognitive evolution</em> как нельзя лучше объясняет суть проблемы. LLM-подход абсолютно чужд нашему разуму, поскольку не имеет с разумом ничего общего кроме поверхностного сходства. Это значит, что ни о каком выравнивании с человеческими ценностями здесь не может идти и речи по определению.</p>
        <p>Тем не менее, Суцкевер верит (по крайней мере, он так утверждает), что этот подход способен привести нас к созданию AGI:</p>
        <p><em>This technology is also going to be different from technology that we are used to,&nbsp;because it will have the ability to improve itself. It is possible to build an AGI that will work on the next generation of AGI.</em></p>
        <p>Вероятно, так и будет. Но реализация этого принципа на основе LLM означает попытку сделать &laquo;лучше&raquo; систему, являющуюся неразумной, ненадёжной и подверженной приступам шизофрении в силу самой своей природы. Это &mdash; всё равно, что надеяться на обретение социального интеллекта аутистом за счёт &laquo;улучшения&raquo; его аутических характеристик.</p>
        <h3>Почему безопасность важна только на словах</h3>
        <p>Наконец, Суцкевер касается вопроса безопасности AGI:</p>
        <p><em>In addition to working with governments and helping them understand what is coming and prepare for it, we are also doing a lot of research on addressing the technological side of things, so that the AI will never want to go rogue. And this is something which I&rsquo;m working on as well.</em></p>
        <p>У нас нет оснований сомневаться в его искренности. К сожалению, это не означает ни эффективности, ни прозрачности соответствующих усилий со стороны разработчиков LLM-продуктов. То, что происходит сейчас, сигнализирует об обратном. Релизы ИИ-агентов происходят один за другим при том, что до сих пор не видно и намёка на устранение типичных для этих продуктов пороков. Так же отсутствует информирование со стороны разработчиков ИИ общественности о каких-либо стратегиях обеспечения безопасности.</p>
        <p>В то же время, сервисы, обещающие сгенерировать за несколько минут <a href="https://aithor.com/"><em>эссе на любую тему</em></a> или выполнить <a href="https://sakana.ai/ai-scientist/"><em>Fully Automated Open-Ended Scientific Discovery</em></a> растут как грибы после дождя. Более того, на GitHub выложено множество разработок так наз. Autonomous LLM agent, способных самостоятельно выполнять задачи без постоянного вмешательства человека. Они понимают инструкции на естественном языке и могут принимать решения на основе своих выводов. Также, они могут создавать субагентов и назначать им задачи. Некоторые из них, например AutoGPT имеет опцию корректировки своих действий на основе анализа промежуточных результатов.</p>
        <p>Иными словами, они пытаются действовать как разумные агенты при том, что не обладают ничем, похожим на разум и склонны к галлюцинациям так же, как и ChatGPT.</p>
        <p>Никто не несёт ответственности за конечные результаты подобного использования ИИ-систем, не говоря уже за долгосрочные последствия. Реальные меры предосторожности пока что заканчиваются на предупреждении, что &ldquo;ChatGPT (или другой ИИ-агент) can make mistakes&rdquo;. Для OpenAI, Anthropic и других этого достаточно, чтобы чувствовать себя придерживающимися норм корпоративной социальной ответственности. И они будут верить в это до тех пор, пока будут достигать своей реальной, а не декларируемой цели. Эта цель очевидна: лидерство в ИИ-гонке. Это &mdash; естественная цель для любого коммерческого предприятия в условиях свободного рынка. Она диктуется правилами этой системы, и все её участники вынуждены им подчиняться. Проблема здесь в том, что следование этим правилам без контроля состояния гонки со стороны общественности приведёт ко всеобщей катастрофе.</p> 
        <h2>Don&rsquo;t Look Up</h2>
        <p>То, что на основе LLM нельзя создать AGI, некоторых может успокоить. Большинство по-прежнему считает самой большой угрозой злонамеренный Суперинтеллект. Отчасти это правда, потому что у LLM-агента намерений быть не может в принципе. Но это не значит, что от него не может исходить экзистенциальной опасности, тем более, когда такая компания, как OpenAI, прямо заявляет, что её целью является <a href="https://openai.com/charter/">создание AGI</a>.</p>
        <p>Возможно, что Сэм Альтман понимает или догадывается, что на самом деле эта цель недостижима. Возможно, об этом догадывается и Илья Суцкевер. Но это не столь важно, сколь может показаться обычному здравомыслящему человеку. OpenAI и другие участники гонки будут пытаться <em>имитировать</em> AGI, чтобы не потерять интерес и доверие инвесторов. Доверие, в свою очередь, гораздо больше зависит от мнения пользователей, нежели от концептуальной обоснованности подхода к разработке продукта. Тем самым, пока юзерам будет удаваться внушить, что LLM-агенты делают нечто невероятное и доселе неведомое, этот пузырь будет раздуваться. Массированные инвестиции, в свою очередь, позволят в какой-то степени улучшать поведение этих продуктов, не затрагивая фундаментально порочный подход. Возможно, будут найдены некоторые остроумные и даже блестящие решения, объединяющие LLM с другими подходами, и позволяющие смягчать приступы шизофрении ИИ-агентов или эффективно маскировать их. В лучшем случае этот гибридный подход со временем отодвинет LLM на второй план или даже позволит вовсе от него отказаться. Может быть, через некоторое, уже непродолжительное, время, произойдёт очередной сдвиг парадигмы и в области ИИ появятся другие лидеры. Или нынешние лидеры без лишнего шума переключатся на эту новую парадигму, не меняя своих лозунгов. В бурно развивающемся мире ИИ возможно всё.</p>
        <p>Но что, если роковой сбой случится до того, как публика и инвесторы протрезвеют и поймут, что наращивание мощности LLM не только тупиковый, но и &mdash; опасный &mdash; путь? Что, если отказ признать этот факт выльется в создание какого-нибудь ИИ-Франкенштейна? Он может выглядеть симпатично и до поры до времени радовать пользователей постоянно улучшающимися результатами. А потом у него случится очередной (и, конечно &mdash; непредсказуемый) приступ, результатом чего станет коллапс всей системы с катастрофическими для человечества последствиями.</p>
        <p>К сожалению, в нашей истории было множество катастроф, которых люди могли бы избежать, если бы руководствовались доброй волей и здравым смыслом. Хотя было бы несправедливым утверждать, что мы не делаем выводов из нашей истории, следует принять во внимание то, что эта проблема беспрецедентно сложна из-за быстро изменяющегося состояния дел в области ИИ. И хотя в её решении задействована масса чрезвычайно умных и ответственных людей, это не гарантирует принятия правильных решений, включая те, что покажутся этим людям очевидными. Их намерениям могут противостоять сиюминутные корпоративные интересы (политические, финансовые или идеологические) и такая непреодолимая сила, как коллективная глупость людей, сцементированная их убеждённостью в собственной правоте.</p>
        <p>Очевидно, до тех пор, пока общественность не заставит регулирующие органы вмешаться в ход ИИ-гонки, её участники будут следовать проверенному принципу <em>Don&rsquo;t&nbsp;Look&nbsp;Up</em>. И не важно кто эти участники, поскольку, как мы заметили, таковы правила игры и все, кто хочет в ней остаться, не имеют иной опции, нежели им подчиниться. Единственный выход &mdash; внести изменения в сами эти правила.</p>
        <h2>Давление геополитического фактора</h2>
        <p>У проблемы ИИ-гонки есть ещё один объективный фактор &mdash; противостояние Запада и Китая. Тот, кто первым создаст дружественный для себя Суперинтеллект, по сути сможет диктовать свою волю остальному миру. Станет ли это делать Запад &mdash; это один вопрос, но что касается Китая, то есть все основания ожидать от него именно этого.</p>
        <p>Китай не является демократией. По сути, он является квази-тоталитарным государством, стремящимся стать тоталитарным полностью. Для этого он полагается на высокие технологии, в частности, на систему так наз. &laquo;Социального кредита&raquo;. Эта система, конечно, не угрожает Западу, но она ясно свидетельствует о намерениях китайского руководства, которое, фактически, уже превратилось в единоличное правление Си Цзиньпина. Это намерение очень простое &mdash; обретение максимальной власти и контроля над всеми остальными.</p>
        <p>Кроме того, Китай отлично помнит то унижение, которому он подвергся со стороны Запада в 19&nbsp;веке и не скрывает своего желания &laquo;восстановить справедливость&raquo;. Несложно понять, как он это понимает &mdash; как уничтожение или подчинение Запада. И если ему удастся создать Суперинтеллект первым, у него будет прекрасный шанс осуществить своё намерение.</p>
        <p>Эта опасность вносит свои коррективы в динамику ИИ-отрасли. Хотя не все эксперты согласны с тем, что от Китая здесь исходит реальная угроза, она, тем не менее, в той или иной степени присутствует. Это значит, что &laquo;поставить разработку ИИ на паузу&raquo; может оказаться затруднительным.</p>
        <p>Тем самым Западное общество может оказаться между молотом и наковальней. С одной стороны, неконтролируемое развитие ИИ может привести к катастрофе возникновения недружественного Суперинтеллекта. С другой стороны, уделение недостаточного внимания разработке дружественного Суперинтеллекта может привести к катастрофе создания такового Китаем.</p>
        <p>Т.о., Западный мир так или иначе оказывается перед экзистенциальным выбором, ответ на которые потребуется уже в ближайшие годы.</p>
        <h2>Что же нам делать?</h2>
        <p>Итак, Великий Фильтр в виде грядущего пришествия Суперинтеллекта &mdash; это вызов, которые нам следует взять в расчёт. Но до того, как мы столкнёмся с ним лицом к лицу, нам предстоит решить проблему выбора правильного пути для его создания.</p>
        <p>Преодоление ложного подхода на основе LLM будет свидетельством нашей способности ответственно подойти к созданию дружественного Суперинтеллекта. В этом смысле можно сказать, что для того, чтобы преодолеть барьер Великого Фильтра нам сначала следует пройти Средний Фильтр. Для этого, в свою очередь, нам нужно преодолеть несколько малых фильтров &mdash; добиться общественного понимания сути проблемы, выработать регуляторные нормы к разработке ИИ-система и разработать стратегию контроля за этим процессом.</p>
        <p>Разумеется, что после этого всего останется масса других, чрезвычайно серьёзных вопросов для решения. Создание истинного Суперинтеллекта, т.е., системы способной к резонированию, &mdash; это беспрецедентная по своей сложности задача. Она включает в себя массу технических, философских и научно-теоретических вопросов. На большинство из них у нас до сих пор нет ответов, хотя уже есть некоторые гипотезы.</p>
        <p>Далее мы погрузимся в углублённое исследование рисков создания Суперинтеллекта и попытаемся выяснить, может ли он быть для нас безопасным. Достоверные ответы на поставленные в этих разделах вопросы позволят нам выносить суждение о том, насколько велики наши шансы преодолеть гипотетический Великий Фильтр, по мере приближения к созданию Суперинтеллекта становящийся всё более осязаемой проблемой.</p> 
      </>
      );
}
