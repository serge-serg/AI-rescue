import Link from 'next/link'
import Image from 'next/image'
import { generateMetadata } from '@/utils/generateMetadata';
export const metadata = generateMetadata(__filename);
import PageWrapper from '@/components/PageWrapper';
import Tooltip from '@/components/Tooltip';
import imgBostrom from '@/assets/images/bostrom.jpg'
import imgMusk from '@/assets/images/musk-3.jpg'
import imgKurzweil from '@/assets/images/kurzweil.jpg'
import imgGood from '@/assets/images/IJ-Good.png'

export default function Home() {
  return (
    <PageWrapper filename={__filename}>
      <h2>Why Everything <span className='accent'>Has Changed</span> So Much</h2>
      <p>
        With the emergence of ChatGPT at the close of 2022, AI transitioned from the realm of science fiction to a rapidly approaching reality. The consensus among AI specialists is now firm-the creation of machines capable of human-level thinking, and even surpassing it, is not a matter of if, but when.
      </p>
      <p>
        <Image
          alt="Bostrom"
          title='Nick BÃ¸strom, "Superintelligence: Paths, Dangers, Strategies"'
          src={imgBostrom}
          width="394"
          className="float-to-right obscured"
        />
        With the emergence of ChatGPT at the close of 2022, AI transitioned from the realm of science fiction to a rapidly approaching reality. The consensus among AI specialists is now firm-the creation of machines capable of human-level thinking, and even surpassing it, is not a matter of if, but when.</p>
      <p>
        AI that has reached such a level of development is called Superintelligence. This term was popularized by Swedish philosopher N.&nbsp;Bostrom in his 2014&nbsp;book
        <em><strong>Superintelligence: Paths, Dangers, Strategies</strong></em>.
        He and many other thinkers believe that Superintelligence will be capable of recursive self-improvement, and this process will be impossible to control externally.
      </p>
      <p>
        It is still unknown when AI of this kind will be created, but the speed at which this direction of scientific and technological progress is developing suggests that it will take years, not decades.
      </p>
      <h2>What is the <span className='accent'>problem</span> here?</h2>
      <p>
        The problem here is that we may create an entity more powerful than ourselves and with goals different from ours. This means that the existence of humanity may depend on its intentions. If these intentions are friendly, Superintelligence could become our partner. This would allow us to solve the fundamental problems that humanity has been grappling with for millennia. Perhaps the very form of our existence will change, turning into a symbiosis of human and artificial minds. That will open up prospects for us to understand the universe at a level of comprehension that we cannot even imagine right now. Although it&apos;s impossible to predict what this future will be like, our main hope here is that it will be the subject of our desire and choice.
      </p>
      <p>
        In the worst-case scenario, we will disappear from the historical scene as a species. We may become unnecessary to this super-powerful entity. It may happen that we, in accordance with Elon Musk&apos;s concern, will simply end up as a &quot;biological bootloader for Superintelligence.&quot; Thus, the latter may turn out to be what journalist and writer J.&nbsp;Barrat called &quot;Our Final Invention&quot; in his 2013 book (of the same name)&mdash;an invention that was created in a <em>bad</em> sense.
      </p>
      <h2><Image
        alt="Musk"
        title='Elon Musk'
        src={imgMusk}
        width="430"
        className="float-to-left obscured"
      />
        Why We <span className='accent'>Won&apos;t Be Able</span> to Cancel the Creation of Superintelligence</h2>
      <p>
        The fundamental aspect of this problem is that we won&apos;t be able to refuse to create this entity. Slowing down this process is also very difficult. There are several objective reasons for this. We analyze them in detail in the section <Link href="/why-we-will-not-refuse-creating-superintelligence">Why We Won&apos;t Refuse Creating Superintelligence</Link>. Here, we&apos;ll mention the most obvious ones:
      </p>
      <h3>1. Market Demand for AI Products</h3>
      <p>
        The demand for AI means an AI race. Regulating this process is extremely difficult because public awareness of the problem lags behind the dynamics of its development. This makes it difficult to make effective political decisions in this area.
      </p>
      <h3>2. Business Priorities of AI Developers</h3>
      <p>
        AI development companies are primarily interested in making a profit, not in ensuring the safety of their products. This prompts them to sacrifice the latter for the former. Even if they are aware of the possible consequences of such an approach, it&apos;s difficult for them to make balanced decisions due to competitive pressure.
      </p>
      <h3>
        3. Geopolitical Rivalry of States Capable of Creating Superintelligence
      </h3>
      <p>
        A regime intolerant of its ideological opponents may be tempted to use AI to eliminate them. The opposing side, in turn, will take retaliatory measures. Thus, the AI race will be escalated to an international level. This will not only complicate the solution of the safe AI problem but will significantly complicate it.
      </p>
      <p>
        Formal agreement between parties not to use AI as a weapon may prove ineffective. Its observance requires transparency not only of the parties&apos; intentions but also of the state of their developments. This is difficult to control technically and institutionally due to the clash of interests of various parties within societies themselves.
      </p>
      <h3>4. Difficulties in Controlling Illegal AI Development</h3>
      <p>
        Finally, private AI developers, including malicious ones, can avoid any external control. They don&apos;t need any special infrastructure, organization of ultra-complex production, or logistical processes. For the most part, they need access to knowledge and relevant services, which in modern conditions is quite feasible with even relatively modest funding. Obviously, for such developers, safety will not be a priority. The results of their activities may have unintended consequences, catastrophic for all of humanity.
      </p>
      <h2>
        Approaching the Point of <span className='accent'>No Return</span>
        <Image
          alt="kurzweil"
          title='Ray Kurzweil "The Singularity is Near"'
          src={imgKurzweil}
          width="500"
          className="float-to-right obscured"
        />
      </h2>
      <div>
        So, we have reason to believe that we are approaching an event that will be a point of no return in human history. The creation of Superintelligence will cause what British mathematician I.J.&nbsp;Good
        <Tooltip content={
          <>
            <p><strong>Irving John Good</strong>&nbsp;(1916-2009), a British mathematician, statistician, and cryptographer, played a significant role in the development of ideas related to artificial intelligence (AI). His contributions to this field are impressive, especially considering his pioneering work in the mid-20th century.</p>
            <figure>
              <Image style={{ width: "300px", height: "auto" }} src={imgGood} alt="Irving John Good" />
              <figcaption>Irving John Good</figcaption>
            </figure>

            <h4>Key Aspects of His Contribution:</h4>
            <ol>
              <li><strong>Codebreaking Work During World War II</strong>: Good worked at the famous Bletchley Park in the United Kingdom, where he was involved in cryptography and helped break the ciphers used by Nazi Germany, including the Enigma code. His work made a significant contribution to the success of the Allies in the war.</li>
              <li><strong>Pioneering Work in Bayesian Statistics</strong>: Good made a significant contribution to the development of Bayesian statistics, which later became the basis for many modern methods of machine learning and AI. His work in this area continues to influence the current development of algorithms and statistical models used in AI.</li>
              <li><strong>The Concept of the &quot;Intelligence Explosion&quot;</strong>: Good was the first to propose the idea that once a sufficiently powerful AI is created, it could continue to create increasingly powerful versions of itself, leading to a rapid increase in intelligence and technological capabilities. This idea has had a significant impact on subsequent theoretical and practical research in AI.</li>
            </ol>
            <p>Good&apos;s work has inspired many followers and researchers, such as Ray Kurzweil and Nick Bostrom, who have continued to develop his ideas on singularity and the potential threats associated with the development of superintelligence. His ideas remain relevant today, shaping discussions about the future of AI and its impact on humanity.</p>
          </>
        }
        />
        defined as an &quot;intelligence explosion&quot;
        <Tooltip content={<>In his article <a target='_blank' href='https://ouci.dntb.gov.ua/en/works/4aXrrna7/'><em>Speculations Concerning the First Ultraintelligent Machine</em></a>, he described the hypothesis that the creation of an &apos;ultra-intelligent machine&apos; could initiate explosive growth in intelligence, surpassing human capabilities and leading to rapid and uncontrolled technological development.</>} />
        back in the mid-1960s. This term refers to a hypothetical scenario in which an AI, once it reaches a certain level of intelligence, can rapidly improve its intelligence, leading to an exponential increase in its cognitive abilities.
      </div>
      <p>
        At that time, it looked like an indefinitely distant or completely unrealistic prospect. But now it&apos;s not so. The famous inventor, futurist writer, Principal Researcher, and AI Visionary at Google, R.&nbsp;Kurzweil, in his book <em>The&nbsp;Singularity&nbsp;Is&nbsp;Near&nbsp;</em>(2005), predicted the creation of Superintelligence around 2045, and his forecast is now far from the most radical. If this indeed happens, then for the first time in Earth&apos;s history, there will be more than one intelligent species on it, and one of them will be far smarter than the other.
      </p>
      <p>
        The unpleasant assumption from this is that we have no guarantee of survival. Our planning horizon regarding Superintelligence is objectively limited. In the equation of reality with its presence, there are many more unknown variables than those we can operate with. So far, we don&apos;t have reliable approaches to predict its intentions and protect ourselves from those that may pose a threat to us. Thus, we must realize that if we fail to develop such approaches, everything could go according to the worst-case scenario for us.
      </p>
      <h2>How Should We Respond to the Challenge of Superintelligence?</h2>
      <p>
        The good news is that humanity has many brilliant minds ready to work on this issue, vast knowledge, and experience in successfully solving incredibly complex problems. Of course, the problem of Superintelligence is extraordinary in all senses, but this doesn&apos;t mean it&apos;s unsolvable in principle. Besides, our mind has a saving feature of maximum mobilization of its resources in situations of existential challenge.
      </p>
      <p>
        Perhaps overcoming the challenge behind the Superintelligence problem depends most on our ability to realize its relevance.
      </p>
      <p>
        Therefore, we urge everyone concerned about our shared future to engage with the information presented on this site. Your thoughtful consideration and contribution, no matter how small, can make a significant difference in addressing humanity&apos;s most pressing challenge.
      </p>
      <p>So, we will be happy to help you:</p>
      <ul>
        <li>
          Assess the chances that <Link href="/will-superintelligence-become-the-great-filter-for-humanity">Superintelligence will become the Great Filter for Humanity</Link>;
        </li>
        <li>
          Find out why, despite this danger, <Link href="/why-we-will-not-refuse-creating-superintelligence">we won&apos;t refuse to create it</Link>;
        </li>
        <li>
          Understand the details of the <Link href="/deep-dive-into-fundamental-ai-risks">fundamental risks coming from AI</Link>;
        </li>
        {/*<li>
          Imagine Superintelligence not as a threat to our existence, but as <Link href="/can-we-create-inherently-friendly-superintelligence">our partner and friend</Link>;
        </li>*/}
      </ul>
      <p>In addition to this information, you will find sections devoted to the vision of the AI problem through the prism of modern&nbsp;art <Link href="/why-hal-9000-intended-to-kill-all-astronauts-aboard-discovery">[1]</Link> <Link href="/why-could-not-the-matrix-exist-without-humans">[2]</Link>. Finally, we hope to <Link href="/we-need-your-opinion">learn your opinion</Link> on the most pressing issues related to this problem.</p>
      <p>Good luck in exploring the challenge of Superintelligence!</p>
    </PageWrapper>
  )
}